### PROJECT STRUCTURE ###
    agent/
        main.py
    backend/
        config.py
        dependencies.py
        main.py
        utils.py
        detection/
            anomaly_engine.py
            risk_engine.py
            rule_engine.py
            vpn_detector.py
        routers/
            api.py
            auth.py
            collect.py
            policy.py
        services/
            data_service.py
    core/
        config.json
        database.py
        models.py
        security.py
        state.py
            services/
                api.js
    services/
        baseline_service.py
        detector.py
        hostname_resolver.py
        vendor_resolver.py
        ml/
            ml_engine.py
            vpn_ml.py
            models/

==================================================

### FILE: run_agent.py ###
from agent.main import NetworkAgent
import os
from dotenv import load_dotenv

if __name__ == "__main__":
    load_dotenv()
    # Check if core/config.json exists
    config_path = "core/config.json"
    if not os.path.exists(config_path):
        # Fallback for when running from a different folder
        config_path = os.path.join(os.path.dirname(__file__), "core", "config.json")
        
    print(f"[*] Starting SOC Agent using config: {config_path}")
    NetworkAgent(config_path).start()


==================================================

### FILE: run_server.py ###
import uvicorn
from dotenv import load_dotenv

if __name__ == "__main__":
    load_dotenv()
    uvicorn.run("backend.main:app", host="127.0.0.1", port=8000, reload=True)


==================================================

### FILE: agent\main.py ###
import sys
import csv
import os
import threading
import time
import requests
import queue
import socket
import math
import collections
import json
import random
import platform
import signal
import subprocess
import re
import uuid
import psutil
from concurrent.futures import ThreadPoolExecutor
from datetime import datetime, timezone, timedelta
from scapy.all import sniff, IP, DNS, DNSQR, UDP, TCP, Ether
from colorama import Fore, Style, init
import logging

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Add parent directory to sys.path to allow importing modules from root
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# from services.detector import AnomalyDetector

# =========================================================
# THREAD SAFE DEVICE INVENTORY (PERSISTENT)
# =========================================================

class DeviceInventory:
    def __init__(self, storage_file="device_inventory.json"):
        self.lock = threading.Lock()
        self.storage_file = storage_file
        self.devices = {}
        self.load_inventory()
        # Start background save worker (Performance Fix)
        threading.Thread(target=self._auto_save_worker, daemon=True).start()

    def load_inventory(self):
        if os.path.exists(self.storage_file):
            try:
                with open(self.storage_file, "r") as f:
                    self.devices = json.load(f)
                    print(f"{Fore.GREEN}[+] Loaded {len(self.devices)} devices from inventory.")
            except:
                pass

    def _auto_save_worker(self):
        """Batched save every 30 seconds to reduce I/O"""
        while True:
            time.sleep(30)
            self.save_inventory()

    def save_inventory(self):
        try:
            with self.lock: # Ensure thread safety during dump
                with open(self.storage_file, "w") as f:
                    json.dump(self.devices, f)
        except:
            pass

    def update(self, ip, **kwargs):
        with self.lock:
            if ip not in self.devices:
                self.devices[ip] = {
                    "mac": "-",
                    "hostname": "Unknown",
                    "vendor": "Unknown",
                    "os": "Unknown",
                    "type": "Unknown",
                    "confidence": "low",
                    "last_seen": time.time()
                }

            for k, v in kwargs.items():
                if v and v not in ["Unknown", "-"]:
                    self.devices[ip][k] = v

            self.devices[ip]["last_seen"] = time.time()
            # Removed direct save_inventory() call for performance

    def get(self, ip):
        return self.devices.get(ip)


# =========================================================
# MAIN AGENT
# =========================================================

class NetworkAgent:

    def __init__(self, config_path="core/config.json"):
        self.config = self._load_config(config_path)

        url_config = self.config.get("server_url", "http://127.0.0.1:8000")
        if "/api/v1/collect" in url_config:
            base = url_config.split("/api/v1/collect")[0]
        else:
            base = url_config.rstrip("/")

        self.server_url = base + "/api/v1/collect/packet"
        self.batch_url = base + "/api/v1/collect/batch"
        self.heartbeat_url = base + "/api/v1/collect/heartbeat"
        self.policy_url = base + "/api/v1/policy"

        self.agent_id = self._init_agent_id()
        self.organization_id = self.config.get("organization_id", "default-org-id") # Should be provided in config
        self.api_key = os.getenv("AGENT_API_KEY") or self.config.get("api_key", "soc-agent-key-2026")
        self.headers = {"X-API-Key": self.api_key}

        self.policy = {
            "blocked_domains": [],
            "vpn_restriction": False,
            "alert_threshold": 70
        }

        self.batch_size = 10
        self.max_q = 10000
        self.bpf_filter = "udp port 53 or tcp port 53"

        self.is_running = True
        self.remote_active = True
        self.verbose = True
        self.dropped_packets = 0
        
        self.device_inventory = DeviceInventory()
        self.probing_ips = set()
        
        # OUI Vendor Cache
        self.vendor_cache = {
            "00:50:56": "VMware", "00:0C:29": "VMware", "00:05:69": "VMware", 
            "00:1C:14": "VMware", "08:00:27": "Oracle VirtualBox", 
            "00:15:5D": "Microsoft Hyper-V", "DC:A6": "Raspberry Pi",
            "B8:27:EB": "Raspberry Pi", "D8:3A:DD": "Ubiquiti", 
            "F0:9F:C2": "Ubiquiti", "00:11:32": "Synology"
        }

        # Risk Analysis Engine (ML-Enhanced)
        # self.detector = AnomalyDetector(risk_threshold=20)
        
        # Deduplication
        self.dedup_lock = threading.Lock()
        self.recent_queries = {}
        self.dedup_window = 5
        
        # Cleanup Worker
        # Queues and Thread Pools
        self.upload_q = queue.Queue(maxsize=self.max_q)
        self.log_q = queue.Queue(maxsize=self.max_q)
        self.discovery_pool = ThreadPoolExecutor(max_workers=10)

        # Start Workers
        threading.Thread(target=self._dedup_cleanup, daemon=True).start()
        threading.Thread(target=self._upload_worker, daemon=True).start()
        threading.Thread(target=self._heartbeat_worker, daemon=True).start()
        threading.Thread(target=self._discovery_engine, daemon=True).start()
        threading.Thread(target=self._log_worker, daemon=True).start()
        
        self._register_agent()

    def _dedup_cleanup(self):
        while self.is_running:
            time.sleep(60)
            try:
                cutoff = datetime.now(timezone.utc) - timedelta(seconds=self.dedup_window)
                with self.dedup_lock:
                    # Prune old entries
                    keys_to_del = [k for k, v in self.recent_queries.items() if v < cutoff]
                    for k in keys_to_del:
                        del self.recent_queries[k]
            except:
                pass

    def _init_agent_id(self):
        """Ensures a persistent unique Agent ID"""
        id_file = "agent_id.txt"
        if os.path.exists(id_file):
            with open(id_file, "r") as f:
                return f.read().strip()
        
        # If not exists, check config
        cid = self.config.get("agent_id")
        if cid and cid != "GATEWAY_SENSE_01":
            return cid
            
        # Generate new one
        new_id = f"AGENT-{uuid.uuid4().hex[:8].upper()}"
        with open(id_file, "w") as f:
            f.write(new_id)
        return new_id

    def _load_config(self, path):
        try:
            if os.path.exists(path):
                with open(path, "r") as f:
                    return json.load(f)
        except:
            pass
        return {}

    def _register_agent(self):
        retry_delay = 1
        while self.is_running:
            try:
                payload = {
                    "agent_id": self.agent_id,
                    "hostname": socket.gethostname(),
                    "os": platform.system(),
                    "version": "v2.2-professional",
                    "time": datetime.now().isoformat(),
                    "organization_id": self.organization_id
                }
                r = requests.post(self.server_url.replace("/packet", "/register"), json=payload, headers=self.headers, timeout=2)
                r.raise_for_status()
                print(f"{Fore.GREEN}[+] Professional Agent Registered Successfully")
                return
            except Exception as e:
                logger.warning(f"Registration failed: {e}. Retrying in {retry_delay}s...")
                time.sleep(retry_delay)
                retry_delay = min(retry_delay * 2, 30)

    # ======================== UTILS ==========================

    def detect_os(self, ttl):
        if ttl <= 64: return "Linux/Android"
        if ttl <= 128: return "Windows"
        return "Network Device"

    def resolve_vendor(self, mac_address):
        if not mac_address or len(mac_address) < 8: return "Unknown"
        mac_upper = mac_address.upper().replace("-", ":")
        prefix = mac_upper[:8]
        if prefix in self.vendor_cache: return self.vendor_cache[prefix]
        # Fallback to API in future if needed
        return "Unknown"

    def detect_device_type(self, os_fam, vendor, domains):
        vendor = vendor.lower()
        if "apple" in vendor or "samsung" in vendor: return "Mobile/Tablet"
        if "synology" in vendor or "qnap" in vendor: return "NAS"
        if "printer" in vendor or "epson" in vendor or "hp" in vendor: return "Printer"
        if "vmware" in vendor or "oracle" in vendor: return "Virtual Machine"
        return "Unknown"



    # ======================== DISCOVERY ======================

    def _async_enrich(self, ip):
        if ip in self.probing_ips: return
        # Limit to local private ranges
        if not ip.startswith(("192.168.", "10.", "172.")): return
        self.probing_ips.add(ip)
        
        # Submit NetBIOS probe to thread pool
        if platform.system() == "Windows":
            self.discovery_pool.submit(self._probe_netbios, ip)

    def _probe_netbios(self, ip):
        try:
            output = subprocess.check_output(f"nbtstat -A {ip}", shell=True, timeout=2).decode(errors="ignore")
            match = re.search(r'([A-Z0-9\-]+)\s+\<00\>\s+UNIQUE', output)
            if match:
                self.device_inventory.update(ip, hostname=match.group(1), confidence="high")
        except: pass
        finally:
            self.probing_ips.discard(ip)

    def _parse_arp_table(self):
        """Cross-platform ARP table parsing"""
        sys = platform.system()
        arp_map = {}
        
        try:
            if sys == "Windows":
                cmd = "arp -a"
                regex = r'(\d+\.\d+\.\d+\.\d+)\s+([0-9a-f\-]{17})'
            else: # Linux/MacOS
                cmd = "arp -a" if sys == "Darwin" else "arp -n"
                # Linux: 1.2.3.4            ether   00:00:00:00:00:00   C                     eth0
                # Mac: ? (1.2.3.4) at 00:00:00:00:00:00 on en0 ifscope [ethernet]
                regex = r'\(?(\d+\.\d+\.\d+\.\d+)\)?.*([0-9a-f:]{17})'

            output = subprocess.check_output(cmd, shell=True).decode(errors="ignore")
            
            for line in output.split('\n'):
                match = re.search(regex, line, re.I)
                if match:
                    ip, mac = match.groups()
                    mac = mac.replace('-', ':').lower()
                    arp_map[ip] = mac
                    
        except Exception as e:
            logger.debug(f"ARP parse error: {e}")
            
        return arp_map

    def _discovery_engine(self):
        print(f"{Fore.BLUE}[!] Enterprise Discovery Engine: ONLINE")
        # Self-identification
        try:
            self_mac = ':'.join(['{:02x}'.format((uuid.getnode() >> ele) & 0xff) for ele in range(0,48,8)][::-1])
            self.device_inventory.update(self._get_local_ip(), mac=self_mac, hostname=socket.gethostname(), vendor="Self", confidence="high")
        except: pass

        while self.is_running:
            try:
                # ARP Scan
                arp_data = self._parse_arp_table()
                for ip, mac in arp_data.items():
                    vendor = self.resolve_vendor(mac)
                    self.device_inventory.update(ip, mac=mac, vendor=vendor)
            except: pass
            
            time.sleep(60) # Adaptive interval can be added here

    def _get_local_ip(self):
        try:
            s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
            s.connect(("8.8.8.8", 80))
            ip = s.getsockname()[0]
            s.close()
            return ip
        except: return "127.0.0.1"

    # ======================== WORKERS ========================

    def get_device_label(self, ip, is_local_ip=False, is_response=False):
        if not hasattr(self, 'device_inventory'):
             return f"{Style.DIM}{ip}{Style.RESET_ALL}"
             
        dev = self.device_inventory.get(ip)
        if dev and dev["hostname"] != "Unknown":
            label = dev["hostname"]
            if dev["confidence"] == "high":
                return f"{Fore.GREEN}{label}{Style.RESET_ALL}"
            return f"{Fore.CYAN}{label}{Style.RESET_ALL}"
        
        # If gateway
        if ip.endswith(".1") or ip.endswith(".254"):
            return f"{Fore.MAGENTA}GATEWAY{Style.RESET_ALL}"
            
        return f"{Style.DIM}{ip}{Style.RESET_ALL}"

    def process_packet(self, packet):
        if not self.remote_active: return

        # DEBUG: Print dot for every packet to verify capture
        print(".", end="", flush=True) 
                
        try:
            # Check for candidate DNS packets that are being missed
            # if packet.haslayer(UDP) and (packet[UDP].sport == 53 or packet[UDP].dport == 53):
            #    if not packet.haslayer(DNS):
            #        print(f"UDP 53: {packet.summary()}")
            
            if packet.haslayer(DNS) and packet.haslayer(IP) and packet.haslayer(DNSQR):
                # Sampling logic removed or fixed
                # if random.random() > 0.5: return 

                now_utc = datetime.now(timezone.utc)
                src_ip = packet[IP].src
                dst_ip = packet[IP].dst
                is_response = (packet[DNS].qr == 1)
                
                try: domain = packet[DNSQR].qname.decode(errors="ignore").strip(".").lower()
                except: return

                if not domain or len(domain) > 255: return
                
                # Policy Enforcement: Blocked Domains
                if domain in self.policy.get("blocked_domains", []):
                    if self.verbose:
                        print(f"{Fore.RED}[BLOCK]{Style.RESET_ALL} Prevented access to {domain}")
                    return

                port = 53
                proto = "UDP"
                if packet.haslayer(TCP): proto = "TCP"; port = packet[TCP].dport
                elif packet.haslayer(UDP): port = packet[UDP].dport

                # Deduplication
                key = (src_ip, domain, now_utc.second)
                with self.dedup_lock:
                    if key in self.recent_queries: return
                    self.recent_queries[key] = now_utc

                # Enrichment
                ttl = packet[IP].ttl
                os_family = self.detect_os(ttl)
                
                # Device Inventory (Disabled for now to prevent crash)
                # device = self.device_inventory.get(src_ip)
                device = None
                
                if not device:
                    # self._async_enrich(src_ip)
                    mac, vendor, name, conf = "-", "Unknown", "Unknown", "low"
                    d_type = self.detect_device_type(os_family, vendor, [domain])
                else:
                    mac = device["mac"]
                    vendor = device["vendor"]
                    name = device["hostname"]
                    conf = device["confidence"]
                    d_type = device["type"]

                # mDNS/NBNS snooping
                potential_name = None
                if domain.endswith(".local") or ".nbns" in domain:
                    potential_name = domain.split('.')[0].upper()
                    # self.device_inventory.update(src_ip, hostname=potential_name, confidence="high")

                name = potential_name or name
                
                # ML-Enhanced Analysis (Disabled)
                # risk_score, entropy_val, severity, ml_prob = self.detector.analyze_packet(domain, src_ip, 0)
                risk_score, entropy_val, severity, ml_prob = 0, 0.0, "LOW", 0.0

                risk_label = f"{Fore.RED}[{severity}]{Style.RESET_ALL}" if severity != "LOW" else ""

                record = {
                    "time": now_utc.strftime("%Y-%m-%d %H:%M:%S"),
                    "src_ip": src_ip, 
                    "dst_ip": dst_ip,
                    "domain": domain,
                    "protocol": proto,
                    "size": len(packet),
                    "port": str(port),
                    "risk_score": risk_score,
                    "entropy": round(entropy_val, 2),
                    "severity": severity,
                    "device_name": name,
                    "device_type": d_type,
                    "os_family": os_family,
                    "brand": vendor,
                    "mac_address": mac,
                    "identity_confidence": conf,
                    "organization_id": self.organization_id
                }
                
                # Queueing
                if not self.log_q.full():
                    self.log_q.put([record["time"], src_ip, domain, self.agent_id, severity, risk_score, round(entropy_val, 2)])
                
                if not self.upload_q.full():
                    self.upload_q.put(record)

                if self.verbose:
                    out = self.get_device_label(src_ip)
                    out += f" {Style.BRIGHT}{Fore.YELLOW} -> {domain} {Style.RESET_ALL}"
                    if severity != "LOW": out += f" {risk_label}"
                    if os_family != "Unknown": out += f" {Style.DIM}[{os_family}|{vendor}]{Style.RESET_ALL}"
                    print(out)

        except Exception as e:
            logger.error(f"Packet processing error: {e}")

    def _upload_worker(self):
        batch = []
        last_send = time.time()
        retry_delay = 1  # Start with 1s delay
        
        while self.is_running:
            try:
                # Optimized: Reduce timeout to avoid blocking too long if queue is empty
                record = self.upload_q.get(timeout=0.5)
                batch.append(record)
                self.upload_q.task_done()
            except queue.Empty: 
                pass
            
            # Condition to send: Batch full OR time elapsed (and batch has data)
            if len(batch) >= self.batch_size or (time.time() - last_send > 2 and batch):
                try:
                    r = requests.post(self.batch_url, json=batch, headers=self.headers, timeout=2.0)
                    r.raise_for_status()
                    
                    # Success: Reset batch and retry delay
                    batch = []
                    last_send = time.time()
                    retry_delay = 1 
                except Exception as e:
                    # Failure: Exponential Backoff
                    print(f"Upload failed: {e}. Retrying in {retry_delay}s...")
                    time.sleep(retry_delay)
                    retry_delay = min(retry_delay * 2, 60) # Max 60s wait
                    # Keep batch to retry next loop

    def upload(self, record):
        try: requests.post(self.server_url, json=record, headers=self.headers, timeout=1.0)
        except Exception as e: logger.error(f"Single upload failed: {e}")

    def _heartbeat_worker(self):
        while self.is_running:
            try:
                # Metrics
                cpu = psutil.cpu_percent(interval=None)
                ram = psutil.virtual_memory().percent
                
                payload = {
                    "agent_id": self.agent_id,
                    "status": "online" if self.remote_active else "paused",
                    "dropped_packets": self.dropped_packets,
                    "cpu_usage": cpu,
                    "ram_usage": ram,
                    "inventory_size": len(self.device_inventory.devices),
                    "time": datetime.now().isoformat()
                }
                r = requests.post(self.heartbeat_url, json=payload, headers=self.headers, timeout=2)
                r.raise_for_status()
                logger.debug("Heartbeat sent successfully")
                
                # Periodic Policy Fetch
                self._fetch_policy()
            except Exception as e: 
                logger.warning(f"Heartbeat/Policy fetch failed: {e}")
            
            # Use adaptive sleep or fixed interval
            time.sleep(30)

    def _fetch_policy(self):
        try:
            r = requests.get(f"{self.policy_url}/{self.organization_id}", headers=self.headers, timeout=2)
            if r.status_code == 200:
                new_policy = r.json()
                if new_policy != self.policy:
                    self.policy = new_policy
                    print(f"{Fore.CYAN}[+] Policy Updated: {len(self.policy.get('blocked_domains', []))} blocked domains")
        except:
            pass
            
    def _log_worker(self):
        log_file = "local_capture_log.csv"
        buffer = []
        last_flush = time.time()
        while self.is_running:
            try:
                data = self.log_q.get(timeout=1)
                buffer.append(data)
                self.log_q.task_done()
            except: pass
            
            if len(buffer) >= 20 or (time.time() - last_flush > 5 and buffer):
                try:
                    with open(log_file, "a", newline="") as f:
                        writer = csv.writer(f)
                        writer.writerows(buffer)
                    buffer = []
                    last_flush = time.time()
                except Exception as e:
                    logger.error(f"Log flush failed: {e}")

    def stop(self, signum=None, frame=None):
        print(f"\n{Fore.YELLOW}[!] Shutting down Enterprise Agent...")
        self.device_inventory.save_inventory()
        self.is_running = False
        time.sleep(1)
        sys.exit(0)

    def start(self):
        from scapy.all import conf
        print(f"{Fore.CYAN}[*] Available Interfaces:")
        print(conf.iface)
        
        print(f"{Fore.BLUE}{'='*80}\n   ELITE SOC AGENT: ENTERPRISE EDITION | Filter: {self.bpf_filter}\n{'='*80}")
        print(f"{Fore.GREEN}[*] Sniffing started... Generate DNS traffic to see output.")
        # Try without BPF filter first to debug VLAN issues
        sniff(filter=self.bpf_filter, prn=self.process_packet, store=False, promisc=True)

if __name__ == "__main__":
    NetworkAgent("core/config.json").start()


==================================================

### FILE: backend\config.py ###
import os

# CRITICAL: Mandatory environment variables for production security
SECRET_KEY = os.environ["NETVISOR_SECRET_KEY"]
AGENT_API_KEY = os.environ["AGENT_API_KEY"]


# Default State
# State is now managed in core.state



==================================================

### FILE: backend\dependencies.py ###
from fastapi import Request, HTTPException, Depends, status
from core.database import get_db_connection
import logging
import time

logger = logging.getLogger("netvisor.deps")

def get_current_user(request: Request):
    user_id = request.session.get("user_id")
    if not user_id:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED, 
            detail="Session expired or not authenticated"
        )
    return {
        "user_id": user_id,
        "username": request.session.get("username"),
        "role": request.session.get("role"),
        "organization_id": request.session.get("organization_id")
    }

def require_super_admin(user: dict = Depends(get_current_user)):
    if user["role"] != 'super_admin':
        logger.warning(f"SuperAdmin access denied for {user['username']}")
        raise HTTPException(status_code=403, detail="Super Admin access required")
    return user

def require_org_admin(user: dict = Depends(get_current_user)):
    if user["role"] not in ['super_admin', 'org_admin']:
        logger.warning(f"OrgAdmin access denied for {user['username']}")
        raise HTTPException(status_code=403, detail="Organization Admin access required")
    return user

def admin_required(user: dict = Depends(get_current_user)):
    """Legacy compatibility - maps to org_admin in new system"""
    return require_org_admin(user)

def login_required(user: dict = Depends(get_current_user)):
    return user["username"] # Compatibility with existing code

# --- SIMPLE RATE LIMITER ---
last_requests = {}

def rate_limit(seconds_between: float = 0.1):
    def dependency(request: Request):
        client_ip = request.client.host
        now = time.time()
        if client_ip in last_requests:
            if now - last_requests[client_ip] < seconds_between:
                raise HTTPException(status_code=429, detail="Too many requests")
        last_requests[client_ip] = now
        return True
    return dependency


==================================================

### FILE: backend\main.py ###
from fastapi import FastAPI, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.responses import RedirectResponse, FileResponse, JSONResponse
from contextlib import asynccontextmanager
from starlette.middleware.sessions import SessionMiddleware
from colorama import Fore
import asyncio
import os
import socketio
import logging
import time
from dotenv import load_dotenv

load_dotenv()

# --- Centralized Logging ---
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(name)s: %(message)s"
)
logger = logging.getLogger("netvisor")

# --- Socket.IO Server ---
sio = socketio.AsyncServer(async_mode='asgi', cors_allowed_origins='*')

from core.database import init_db, db_writer_worker
from .config import SECRET_KEY
from .routers import auth, collect, api
from core.state import state

# --- Global Exception Handler ---
async def global_exception_handler(request: Request, exc: Exception):
    logger.error(f"Global Exception: {str(exc)}", exc_info=True)
    return JSONResponse(
        status_code=500,
        content={"status": "error", "message": "An internal server error occurred.", "details": str(exc)}
    )

@asynccontextmanager
async def lifespan(app: FastAPI):
    # STARTUP
    state.start_time = time.time()
    init_db()
    asyncio.create_task(db_writer_worker())
    
    # Start periodic baseline computation (every 1 hour)
    async def baseline_timer():
        from services.baseline_service import baseline_service
        while True:
            baseline_service.compute_all_baselines()
            await asyncio.sleep(3600)
            
    asyncio.create_task(baseline_timer())
    
    if not os.path.exists("data/backups"):
        os.makedirs("data/backups")
        
    print(f"{Fore.GREEN}[+] Netvisor Server Industrial Layer is online.")
    yield
    # SHUTDOWN
    print(f"\n{Fore.YELLOW}[!] Server stopping...")
    from core.database import drain_packet_queue
    from .services.data_service import export_to_csv_task, truncate_data
    
    await drain_packet_queue()
    
    print(f"{Fore.CYAN}[*] Archiving session data...")
    csv_file = export_to_csv_task()
    if csv_file and csv_file != "empty":
        print(f"{Fore.GREEN}[+] Data backed up: {csv_file}")
        if truncate_data():
            print(f"{Fore.GREEN}[+] Database truncated for next run.")
        else:
            print(f"{Fore.RED}[X] Failed to truncate database.")
    else:
        print(f"{Fore.YELLOW}[!] No data to archive or export failed.")

app = FastAPI(
    title="Netvisor | Industrial SOC", 
    lifespan=lifespan,
    exception_handlers={Exception: global_exception_handler}
)

# --- CORS Hardening ---
app.add_middleware(
    CORSMiddleware,
    allow_origins=os.getenv("CORS_ORIGINS", "*").split(","),
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.get("/api/ping")
async def ping():
    return {"status": "pong"}

# Middleware
@app.middleware("http")
async def maintenance_middleware(request: Request, call_next):
    exempt_paths = ["/maintenance", "/login", "/register", "/assets", "/api/v1/collect"]
    if state.maintenance_mode and not any(request.url.path.startswith(p) for p in exempt_paths):
        if "user_id" not in request.session:
            return RedirectResponse(url="/maintenance")
    return await call_next(request)

app.add_middleware(
    SessionMiddleware, 
    secret_key=SECRET_KEY,
    https_only=os.getenv("SECURE_COOKIES", "False").lower() == "true",
    same_site="lax",
    max_age=3600 # 1 hour session expiration for enhanced security
)

# Static Files
app.mount("/assets", StaticFiles(directory="frontend/dist/assets"), name="assets")


# Routers
# NOTE: API routers must be included BEFORE the catch-all frontend route
app.include_router(auth.router)
app.include_router(collect.router)
app.include_router(policy.router)
app.include_router(api.router, prefix="/api")


# Socket.IO Events
@sio.event
async def connect(sid, environ):
    logger.info(f"Socket connected: {sid}")

@sio.event
async def disconnect(sid):
    logger.info(f"Socket disconnected: {sid}")

# Wrap with Socket.IO
# Restrict CORS to specific origins in production
cors_origins = os.getenv("CORS_ORIGINS", "*").split(",")
if len(cors_origins) == 1 and cors_origins[0] == "*":
    cors_origins = "*"

# Helper to serve React Index (Catch-All MUST be last, but before SocketIO wrap)
@app.get("/{full_path:path}")
async def serve_react_app(request: Request, full_path: str):
    if full_path.startswith("api") or full_path.startswith("socket.io"):
        return {"status": "error", "message": "Not Found"}

    if os.path.exists("frontend/dist/index.html"):
        return FileResponse("frontend/dist/index.html")
    return {"status": "error", "message": "Frontend build not found. Run 'npm run build'."}

app = socketio.ASGIApp(sio, app, socketio_path='socket.io')


==================================================

### FILE: backend\utils.py ###
from fastapi import Request

def fastapi_url_for_compat(request: Request):
    def _url_for(name: str, **path_params):
        if name == 'static' and 'filename' in path_params:
            path_params['path'] = path_params.pop('filename')
        return request.url_for(name, **path_params)
    return _url_for


==================================================

### FILE: backend\detection\anomaly_engine.py ###
import time
import collections

class AnomalyEngine:
    def __init__(self):
        # In-memory session state for behavioral tracking
        self.ip_counts = collections.defaultdict(int)
        self.nx_counts = collections.defaultdict(int)
        self.unique_domains = collections.defaultdict(set)
        self.start_time = time.time()

    def analyze_behavior(self, src_ip, domain, rcode=0, baseline=None):
        """
        Stateful analysis compared against baseline.
        Returns: score
        """
        score = 0
        
        # 1. Error Rate Tracking (NXDOMAIN)
        if rcode != 0:
            self.nx_counts[src_ip] += 1
            if self.nx_counts[src_ip] > 10:
                score += 2
        
        # 2. Volume and Variety
        self.ip_counts[src_ip] += 1
        self.unique_domains[src_ip].add(domain)
        
        if len(self.unique_domains[src_ip]) > 50:
            score += 3
            
        # 3. Baseline Comparison (if provided)
        elapsed = time.time() - self.start_time
        if elapsed > 60: # At least 1 minute of data for rate calc
            rate = self.ip_counts[src_ip] / (elapsed / 60) # QPM
            
            if baseline:
                # If rate is 5x the baseline, trigger anomaly
                if baseline.avg_packet_rate > 0 and rate > (baseline.avg_packet_rate * 5):
                    score += 5
            else:
                # Default heuristic if no baseline exists
                if rate > 100: score += 2

        # Reset counters every 5 minutes to prevent memory bloat and stale spikes
        if time.time() - self.start_time > 300:
            self.reset_counters()
            
        return score

    def reset_counters(self):
        self.ip_counts.clear()
        self.nx_counts.clear()
        self.unique_domains.clear()
        self.start_time = time.time()


==================================================

### FILE: backend\detection\risk_engine.py ###
from .rule_engine import RuleEngine
from .anomaly_engine import AnomalyEngine
from .vpn_detector import VPNDetector
from services.ml.ml_engine import ml_engine

class RiskEngine:
    def __init__(self):
        self.rule_engine = RuleEngine()
        self.anomaly_engine = AnomalyEngine()
        self.vpn_detector = VPNDetector()

    def calculate_device_risk(self, domain, src_ip, dst_ip, port, rcode=0, baseline=None):
        """
        Aggregates all findings into a single risk report.
        Formula: risk = (anomaly_score * 0.5 + vpn_score * 0.3 + rule_score * 0.2)
        Wait, I'll stick to the one in the plan or a simplified aggregate.
        """
        # 1. Rule Engine (Lexical)
        rule_score, entropy = self.rule_engine.analyze(domain)
        
        # 2. Anomaly Engine (Behavioral)
        anomaly_score = self.anomaly_engine.analyze_behavior(src_ip, domain, rcode, baseline)
        
        # 3. VPN Detector
        vpn_score, vpn_reason = self.vpn_detector.analyze_vpn(src_ip, dst_ip, port)
        
        # 4. ML Engine
        ml_prob = ml_engine.predict_risk(domain)
        ml_score = 10 if ml_prob > 0.8 else (5 if ml_prob > 0.5 else 0)

        # FINAL AGGREGATION
        # We cap sub-scores or weight them
        # Total score 0-100
        total_score = (
            (anomaly_score * 4) + # max ~40
            (vpn_score * 2) +     # max ~40
            (rule_score * 3) +    # max ~21
            (ml_score * 2)        # max ~20
        )
        
        total_score = min(100, total_score)
        
        reasons = []
        if rule_score > 4: reasons.append("Suspicious Domain Structure (Lexical)")
        if anomaly_score > 4: reasons.append("Anomalous Query Rate/Pattern")
        if vpn_score > 0: reasons.append(vpn_reason or "VPN Tunneling pattern")
        if ml_score > 5: reasons.append("ML Classifier: Malicious Domain")
        
        severity = "LOW"
        if total_score > 70: severity = "HIGH"
        elif total_score > 30: severity = "MEDIUM"
        
        return {
            "score": total_score,
            "severity": severity,
            "reasons": reasons,
            "entropy": entropy,
            "ml_prob": ml_prob
        }

# Singleton
risk_engine = RiskEngine()


==================================================

### FILE: backend\detection\rule_engine.py ###
import collections
import math

class RuleEngine:
    def __init__(self, risk_threshold=6):
        self.risk_threshold = risk_threshold
        # Top benign domains to skip heavy processing
        self.whitelist = {"google.com", "facebook.com", "microsoft.com", "apple.com", "amazon.com"}

    def calculate_entropy(self, text):
        if not text: return 0
        counter = collections.Counter(text)
        total = len(text)
        return -sum((count/total) * math.log2(count/total) for count in counter.values())

    def analyze(self, domain):
        """
        Analyze logic migrated from detector.py
        Returns: (score, entropy)
        """
        score = 0
        parts = domain.split('.')
        longest_part = max(parts, key=len) if parts else domain
        
        if any(w in domain for w in self.whitelist):
            return 0, 0

        # Heuristic 1: Length
        if len(longest_part) > 25: score += 2
        
        # Heuristic 2: Digit Count
        if sum(c.isdigit() for c in longest_part) > 5: score += 2
        
        # Heuristic 3: Entropy
        ent = self.calculate_entropy(longest_part)
        if ent > 4.5: score += 3
        
        return score, ent


==================================================

### FILE: backend\detection\vpn_detector.py ###
import ipaddress

class VPNDetector:
    def __init__(self):
        # Example suspicious ranges (In production, load from a database or API)
        self.suspicious_ranges = [
            # Dummy examples
            "103.1.2.0/24",
            "45.2.3.0/24"
        ]

    def is_suspicious_ip(self, ip):
        """Checks if IP belongs to known VPN/suspicious ranges"""
        try:
            ip_obj = ipaddress.ip_address(ip)
            for network in self.suspicious_ranges:
                if ip_obj in ipaddress.ip_network(network):
                    return True
        except:
            pass
        return False

    def analyze_vpn(self, src_ip, dst_ip, port):
        """
        Returns: score, reason
        """
        score = 0
        reason = ""
        
        # 1. Destination IP Check
        if self.is_suspicious_ip(dst_ip):
            score += 15
            reason = "Traffic to known VPN/Proxy range"
            
        # 2. Port patterns (common VPN ports)
        if port in ["1194", "443", "500", "4500", "1701"]:
            # port 443 is common for everything, so we need more context
            # but 1194 (OpenVPN) or 500/4500 (IPsec) are stronger flags
            if port in ["1194", "500", "4500"]:
                score += 10
                reason = "VPN Protocol Port detected"
                
        return score, reason


==================================================

### FILE: backend\routers\api.py ###
from fastapi import APIRouter, Depends, File, HTTPException, status
from fastapi.responses import FileResponse
from typing import List, Optional
import psutil
import time
import socket
import os
import logging

from ..dependencies import login_required, admin_required
from ..services.data_service import get_stats, fetch_recent_traffic, export_to_csv_task, truncate_data, fetch_system_logs, fetch_vpn_alerts
from core.models import (
    HotspotRequest, SystemConfigRequest, GenericResponse, 
    DeviceResponse, SystemHealthResponse, AdminStatsResponse, 
    ActivityEntry, LogsResponse
)
from core.state import state

logger = logging.getLogger("netvisor.api")
router = APIRouter(tags=["System API"])


@router.get("/logs", response_model=LogsResponse)
async def api_logs():
    admin_logs = fetch_system_logs(limit=20)
    vpn_logs = fetch_vpn_alerts(limit=20)
    
    return {
        "admin": [{
            "time": l["timestamp"],
            "action": l["action"],
            "details": f"User: {l['username']} | IP: {l['ip_address']}"
        } for l in admin_logs],
        "vpn": [{
            "time": l["timestamp"],
            "src_ip": l["source_ip"],
            "score": (l["risk_score"] or 0) / 100.0,
            "reason": f"High Risk Traffic ({l['protocol']})"
        } for l in vpn_logs]
    }

@router.get("/system-health", response_model=SystemHealthResponse)
async def api_health():
    uptime = time.time() - state.start_time
    return {
        "status": "Operational",
        "cpu_usage": psutil.cpu_percent(),
        "ram_usage": psutil.virtual_memory().percent,
        "uptime_hours": round(uptime / 3600, 2)
    }

@router.get("/admin/stats", response_model=AdminStatsResponse)
async def admin_stats_api(username: str = Depends(login_required)):
    return {
        "hostname": socket.gethostname(),
        "local_ip": socket.gethostbyname(socket.gethostname()),
        "cpu_percent": psutil.cpu_percent(),
        "mem_used_mb": psutil.virtual_memory().used / (1024 * 1024),
        "mem_total_mb": psutil.virtual_memory().total / (1024 * 1024),
        "maintenance_mode": state.maintenance_mode
    }

@router.get("/stats")
async def api_stats():
    # Statistics aggregator - returns dynamic dict based on service
    return get_stats()

@router.get("/activity", response_model=List[ActivityEntry])
async def api_activity(severity: Optional[str] = None):
    rows = fetch_recent_traffic(limit=50, severity=severity)
    return [{
        "time": r["timestamp"], 
        "ip": r["source_ip"], 
        "dst_ip": r.get("dst_ip", "-"), 
        "domain": r["domain"], 
        "protocol": r["protocol"], 
        "size": r.get("packet_size", 0),
        "device": r.get("device_name") or "Unknown",
        "os": r.get("os_family") or "Unknown",
        "brand": r.get("brand") or "Unknown",
        "mac": r.get("mac_address", "-"),
        "confidence": r.get("identity_confidence", "low"),
        "severity": r.get("severity", "LOW")
    } for r in rows]

@router.get("/devices", response_model=List[DeviceResponse])
async def api_devices(username: str = Depends(login_required)):
    from ..services.data_service import fetch_device_risks
    
    traffic_rows = fetch_recent_traffic(limit=2000)
    risk_rows = fetch_device_risks()
    
    risks_map = {r['device_id']: r for r in risk_rows}
    
    devices_map = {}
    for row in traffic_rows:
        ip = row.get("source_ip")
        mac = row.get("mac_address", "-")
        if ip and ip not in devices_map:
            # Get risk for this device (by MAC preferred)
            risk = risks_map.get(mac, {})
            
            devices_map[ip] = {
                "ip": ip, "mac": mac, "hostname": row.get("device_name") or "Unknown",
                "traffic": 0.1, "is_online": True, "last_seen": row.get("timestamp"),
                "type": row.get("device_type") or "Unknown",
                "os": row.get("os_family") or "Unknown",
                "brand": row.get("brand") or "Unknown",
                "confidence": row.get("identity_confidence", "low"),
                "risk_score": int(risk.get("current_score", 0)),
                "risk_level": risk.get("risk_level", "LOW")
            }
    return list(devices_map.values())

@router.get("/admin/hotspot/status")
async def hotspot_status(username: str = Depends(login_required)): 
    return {"active": state.hotspot_active}

@router.post("/admin/hotspot", response_model=GenericResponse)
async def toggle_hotspot_api(data: HotspotRequest, username: str = Depends(admin_required)):
    state.hotspot_active = (data.action == 'start')
    return {"status": "success", "message": f"Hotspot {'started' if state.hotspot_active else 'stopped'}"}

@router.get("/settings/system/status")
async def system_status_api(username: str = Depends(login_required)): 
    return {"active": state.monitoring_active}

@router.post("/settings/system", response_model=GenericResponse)
async def toggle_monitoring_api(data: SystemConfigRequest, username: str = Depends(admin_required)):
    state.monitoring_active = data.active
    return {"status": "success", "message": "Monitoring state updated"}

@router.post("/settings/maintenance", response_model=GenericResponse)
async def toggle_maintenance_api(data: SystemConfigRequest, username: str = Depends(admin_required)):
    state.maintenance_mode = data.active
    return {"status": "success", "message": f"Maintenance mode {'enabled' if data.active else 'disabled'}"}

@router.post("/admin/reset_db", response_model=GenericResponse)
async def reset_db_api(username: str = Depends(admin_required)):
    file = export_to_csv_task()
    success = truncate_data()
    msg = "Data reset successfully."
    if file and file != "empty": msg += f" Backup saved to {file}"
    return {"status": "success" if success else "error", "message": msg}

@router.get("/export/devices/{fmt}", name="export_devices")
async def api_export_devices(fmt: str, username: str = Depends(login_required)):
    filename = export_to_csv_task()
    if filename and filename != "empty":
        return FileResponse(path=filename, filename=os.path.basename(filename), media_type='text/csv')
    return JSONResponse(status_code=404, content={"status": "error", "message": "No data to export"})


==================================================

### FILE: backend\routers\auth.py ###
from fastapi import APIRouter, Request, Form, Depends, HTTPException
import time
from fastapi.responses import JSONResponse
from core.database import get_db_connection
from core.security import verify_password, hash_password

from fastapi import APIRouter, Request, Depends, HTTPException, status
import time
from fastapi.responses import JSONResponse
from core.database import get_db_connection
from core.security import verify_password, hash_password
from core.models import UserLogin, UserRegister, GenericResponse
import uuid

router = APIRouter(tags=["Authentication"])

# Simple in-memory rate limiting
login_attempts = {}

@router.post("/login", response_model=GenericResponse)
async def login_handler(request: Request, credentials: UserLogin):
    client_ip = request.client.host
    current_time = time.time()
    
    username = credentials.username
    password = credentials.password

    # Cleanup old attempts
    if client_ip in login_attempts:
        attempts, last_time = login_attempts[client_ip]
        if current_time - last_time > 300: # 5 minutes lockout window
            del login_attempts[client_ip]
    
    if client_ip in login_attempts:
        attempts, last_time = login_attempts[client_ip]
        if attempts >= 5:
            raise HTTPException(status_code=429, detail="Too many failed attempts. Try again later.")

    conn = get_db_connection()
    if not conn: 
        raise HTTPException(status_code=500, detail="Database connection failed")
    
    try:
        cursor = conn.cursor(dictionary=True)
        cursor.execute("SELECT * FROM users WHERE username = %s", (username,))
        user = cursor.fetchone()
        
        if user and verify_password(password, user["password"]):
            # Reset attempts on success
            if client_ip in login_attempts: del login_attempts[client_ip]
            
            request.session["user_id"] = user["id"]
            request.session["username"] = user["username"]
            request.session["role"] = user["role"]
            request.session["organization_id"] = user["organization_id"]
            return {"status": "success", "message": "Login successful"}
    finally:
        if 'cursor' in locals(): cursor.close()
        conn.close()
    
    # Increment failure counter
    attempts, _ = login_attempts.get(client_ip, (0, time.time()))
    login_attempts[client_ip] = (attempts + 1, time.time())
    
    raise HTTPException(status_code=401, detail="Invalid username or password")

@router.post("/register", response_model=GenericResponse)
async def register_handler(data: UserRegister):
    if data.password != data.confirm_password:
        raise HTTPException(status_code=400, detail="Passwords do not match.")
    
    conn = get_db_connection()
    if not conn: 
        raise HTTPException(status_code=500, detail="Database connection failed")

    try:
        cursor = conn.cursor()
        cursor.execute("SELECT id FROM users WHERE username = %s OR email = %s", (data.username, data.email))
        if cursor.fetchone():
            raise HTTPException(status_code=409, detail="Username or Email already exists.")
        
        # Get default organization
        cursor = conn.cursor(dictionary=True)
        cursor.execute("SELECT id FROM organizations WHERE name = 'Default Organization' LIMIT 1")
        org = cursor.fetchone()
        default_org_id = org["id"] if org else None

        hashed = hash_password(data.password)
        user_id = str(uuid.uuid4())
        cursor = conn.cursor()
        cursor.execute(
            "INSERT INTO users (id, username, password, email, role, organization_id) VALUES (%s, %s, %s, %s, %s, %s)", 
            (user_id, data.username, hashed, data.email, data.role or "user", default_org_id)
        )
        conn.commit()
        return {"status": "success", "message": "Registration successful"}
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Registration error: {e}")
        raise HTTPException(status_code=500, detail="An error occurred during registration.")
    finally:
        if 'cursor' in locals(): cursor.close()
        conn.close()

@router.get("/logout")
async def logout(request: Request):
    request.session.clear()
    return {"status": "success", "message": "Logged out"}

@router.post("/forgot-password")
async def forgot_password_handler(request: Request):
    # Simple stub
    return {"status": "success", "message": "If that account exists, a reset link has been sent."}

@router.get("/api/me")
async def check_auth(request: Request):
    user_id = request.session.get("user_id")
    if user_id:
        return {
            "authenticated": True, 
            "username": request.session.get("username"), 
            "role": request.session.get("role"),
            "organization_id": request.session.get("organization_id")
        }
    return {"authenticated": False}



==================================================

### FILE: backend\routers\collect.py ###
from fastapi import APIRouter, HTTPException, Request, Depends, status
from typing import List
import asyncio
import datetime
import logging
from core.models import PacketLog, AgentRegistration, AgentHeartbeat, GenericResponse
from core.database import packet_queue
from ..config import AGENT_API_KEY
from ..dependencies import rate_limit

logger = logging.getLogger("netvisor.collect")
router = APIRouter(prefix="/api/v1/collect", tags=["Agent Collection"])

async def validate_agent_key(request: Request):
    key = request.headers.get("X-API-Key")
    if key != AGENT_API_KEY:
        logger.warning(f"Unauthorized agent access attempt from {request.client.host}")
        raise HTTPException(status_code=status.HTTP_403_FORBIDDEN, detail="Unauthorized Agent Key")
    return True

@router.post("/packet", response_model=GenericResponse)
async def receive_packet_log(
    log: PacketLog, 
    authorized: bool = Depends(validate_agent_key),
    _: bool = Depends(rate_limit(0.05)) # Max 20 packets/sec per agent IP
):
    try:
        packet_queue.put_nowait(log)
        return {"status": "success", "message": "Log buffered"}
    except asyncio.QueueFull:
        logger.error("Packet queue full - dropping log")
        raise HTTPException(status_code=status.HTTP_503_SERVICE_UNAVAILABLE, detail="Buffer full - dropping packet")

@router.post("/batch", response_model=GenericResponse)
async def receive_batch_logs(logs: List[PacketLog], authorized: bool = Depends(validate_agent_key)):
    count = 0
    for log in logs:
        try:
            packet_queue.put_nowait(log)
            count += 1
        except asyncio.QueueFull:
            logger.error(f"Packet queue full during batch - buffered {count}/{len(logs)}")
            break
    
    if count == 0 and len(logs) > 0:
        raise HTTPException(status_code=status.HTTP_503_SERVICE_UNAVAILABLE, detail="Buffer full")
        
    return {"status": "success", "message": f"Buffered {count} logs"}

@router.post("/register", response_model=GenericResponse)
async def register_agent(reg: AgentRegistration, authorized: bool = Depends(validate_agent_key)):
    from core.database import get_db_connection
    conn = get_db_connection()
    if conn:
        try:
            cursor = conn.cursor()
            cursor.execute("""
                INSERT INTO agents (id, name, api_key, organization_id, last_seen)
                VALUES (%s, %s, %s, %s, NOW())
                ON DUPLICATE KEY UPDATE 
                    last_seen = NOW(),
                    name = VALUES(name)
            """, (reg.agent_id, reg.hostname, AGENT_API_KEY, reg.organization_id))
            conn.commit()
            logger.info(f"Agent Registered: {reg.agent_id} | Host: {reg.hostname} | OS: {reg.os}")
            return {"status": "success", "message": "Agent registered successfully"}
        except Exception as e:
            logger.error(f"Registration Error: {e}")
            raise HTTPException(status_code=500, detail="Database registration failed")
        finally:
            conn.close()
    return {"status": "error", "message": "DB connection failed"}

@router.post("/heartbeat", response_model=GenericResponse)
async def agent_heartbeat(hb: AgentHeartbeat, authorized: bool = Depends(validate_agent_key)):
    from core.database import get_db_connection
    conn = get_db_connection()
    if conn:
        try:
            cursor = conn.cursor()
            cursor.execute("UPDATE agents SET last_seen = NOW() WHERE id = %s", (hb.agent_id,))
            conn.commit()
            logger.debug(f"Heartbeat from {hb.agent_id}: CPU {hb.cpu_usage}% | RAM {hb.ram_usage}%")
            return {"status": "success", "message": "Heartbeat received"}
        finally:
            conn.close()
    return {"status": "error", "message": "DB connection failed"}


==================================================

### FILE: backend\routers\policy.py ###
from fastapi import APIRouter, HTTPException, Depends
from core.models import PolicyUpdate, GenericResponse
from core.database import get_db_connection
import json

router = APIRouter(prefix="/api/v1/policy", tags=["Policy Management"])

@router.get("/{org_id}")
async def get_policy(org_id: str):
    conn = get_db_connection()
    if not conn: raise HTTPException(status_code=500, detail="DB Error")
    try:
        cursor = conn.cursor(dictionary=True)
        cursor.execute("SELECT * FROM security_policies WHERE organization_id = %s", (org_id,))
        row = cursor.fetchone()
        if not row:
            # Return defaults
            return {
                "blocked_domains": [],
                "vpn_restriction": False,
                "alert_threshold": 70
            }
        
        # Parse blocked_domains if stored as CSV or JSON
        row['blocked_domains'] = row['blocked_domains'].split(',') if row['blocked_domains'] else []
        return row
    finally:
        conn.close()

@router.post("/update")
async def update_policy(policy: PolicyUpdate):
    conn = get_db_connection()
    if not conn: raise HTTPException(status_code=500, detail="DB Error")
    try:
        cursor = conn.cursor()
        domains_str = ",".join(policy.blocked_domains)
        cursor.execute("""
            INSERT INTO security_policies (organization_id, blocked_domains, vpn_restriction, alert_threshold)
            VALUES (%s, %s, %s, %s)
            ON DUPLICATE KEY UPDATE
                blocked_domains = VALUES(blocked_domains),
                vpn_restriction = VALUES(vpn_restriction),
                alert_threshold = VALUES(alert_threshold)
        """, (policy.organization_id, domains_str, policy.vpn_restriction, policy.alert_threshold))
        conn.commit()
        return {"status": "success", "message": "Policy updated"}
    finally:
        conn.close()


==================================================

### FILE: backend\services\data_service.py ###
from core.database import (
    db_fetch_recent_traffic, db_fetch_system_logs, db_fetch_vpn_alerts,
    db_truncate_tables, db_export_to_csv
)
import datetime
import time
from collections import Counter

# --- In-Memory Cache for Dashboard Stats ---
_cache = {
    "stats": None,
    "expiry": 0
}
CACHE_TTL = 5  # 5 seconds TTL for high-traffic dashboard stats

def export_to_csv_task():
    return db_export_to_csv()

def truncate_data():
    return db_truncate_tables()

def parse_timestamp(value) -> datetime.datetime:
    if isinstance(value, datetime.datetime):
        if value.tzinfo is None:
            value = value.replace(tzinfo=datetime.timezone.utc)
        return value
    return datetime.datetime.now(datetime.timezone.utc)

def fetch_recent_traffic(limit=1000, severity=None, organization_id=None):
    return db_fetch_recent_traffic(limit, severity, organization_id)

def fetch_system_logs(limit=50, organization_id=None):
    return db_fetch_system_logs(limit, organization_id)

def fetch_vpn_alerts(limit=50, organization_id=None):
    return db_fetch_vpn_alerts(limit, organization_id)

def fetch_device_risks(organization_id=None):
    from core.database import db_fetch_device_risks
    return db_fetch_device_risks(organization_id)

def get_stats(organization_id=None):
    global _cache
    now_ts = time.time()
    
    # Simple multi-tenant cache key
    cache_key = f"stats_{organization_id or 'global'}"
    
    if _cache.get(cache_key) and now_ts < _cache.get(f"{cache_key}_expiry", 0):
        return _cache[cache_key]
        
    rows = db_fetch_recent_traffic(limit=1200, organization_id=organization_id)
    protocol_counts = Counter()
    devices = {row.get("source_ip") for row in rows if row.get("source_ip")}
    recent_count = 0
    now = datetime.datetime.now(datetime.timezone.utc)
    
    for row in rows:
        protocol_counts[row.get("protocol") or "DNS"] += 1
        ts = parse_timestamp(row.get("timestamp"))
        if (now - ts).total_seconds() <= 60:
            recent_count += 1
            
    total_mb = sum(row.get("packet_size", 0) for row in rows) / (1024*1024)
    if total_mb == 0 and rows: 
        total_mb = len(rows) * 0.05
        
    stats = {
        "bandwidth": f"{total_mb:.2f} MB",
        "devices": len(devices),
        "vpn_alerts": len([r for r in rows if r.get("severity") == "HIGH"]),
        "protocols": dict(protocol_counts),
        "upload_speed": recent_count * 2,
        "download_speed": recent_count * 5
    }
    
    # Update Cache
    _cache[cache_key] = stats
    _cache[f"{cache_key}_expiry"] = now_ts + CACHE_TTL
    
    return stats


==================================================

### FILE: core\config.json ###
{
  "server_url": "http://127.0.0.1:8000/api/v1/collect/packet",
  "dedup_window": 5,
  "log_file": "local_capture_log.csv",
  "risk_threshold": 5,
  "agent_id": "GATEWAY_SENSE_01",
  "bpf_filter": "udp port 53",
  "heartbeat_url": "http://127.0.0.1:8000/api/v1/collect/heartbeat",
  "verbose": true,
  "max_queue_size": 10000,
  "batch_size": 10,
  "sample_rate": 1.0
}


==================================================

### FILE: core\database.py ###
import mysql.connector
from mysql.connector import pooling
import os
import bcrypt
import asyncio
from colorama import Fore
from .models import PacketLog
import uuid

db_config = {
    "host": os.getenv("NETVISOR_DB_HOST", "localhost"),
    "user": os.getenv("NETVISOR_DB_USER", "root"),
    "password": os.getenv("NETVISOR_DB_PASSWORD", ""),
    "database": os.getenv("NETVISOR_DB_NAME", "network_security"),
}

# Implementation of connection pooling for production-grade stability
try:
    pool = pooling.MySQLConnectionPool(
        pool_name="netvisor_pool",
        pool_size=10,
        **db_config
    )
    print(f"{Fore.GREEN}[+] Managed DB connection pool initialized (Size: 10).")
except Exception as e:
    print(f"{Fore.RED}[X] Failed to initialize connection pool: {e}")
    # Fallback to direct connection if pool fails (though not ideal)
    pool = None

def get_db_connection():
    try:
        if pool:
            return pool.get_connection()
        return mysql.connector.connect(**db_config)
    except Exception as exc:
        print(f"{Fore.RED}[X] DB connection error: {exc}")
        return None

def init_db():
    conn = get_db_connection()
    if conn:
        try:
            cursor = conn.cursor()
            
            # 1. Organizations Table
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS organizations (
                    id CHAR(36) PRIMARY KEY,
                    name VARCHAR(100) NOT NULL,
                    status VARCHAR(20) DEFAULT 'active',
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP
                )
            """)

            # 2. Users Table (Revamped)
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS users (
                    id CHAR(36) PRIMARY KEY,
                    username VARCHAR(50) UNIQUE NOT NULL,
                    password VARCHAR(255) NOT NULL,
                    email VARCHAR(100),
                    role VARCHAR(20) DEFAULT 'user',
                    organization_id CHAR(36),
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                    CONSTRAINT fk_user_org FOREIGN KEY (organization_id) REFERENCES organizations(id)
                )
            """)

            # 3. Agents Table
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS agents (
                    id CHAR(36) PRIMARY KEY,
                    name VARCHAR(100) NOT NULL,
                    api_key TEXT NOT NULL,
                    organization_id CHAR(36),
                    last_seen DATETIME,
                    CONSTRAINT fk_agent_org FOREIGN KEY (organization_id) REFERENCES organizations(id)
                )
            """)

            # 4. Traffic Logs Table (Multi-tenant)
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS traffic_logs (
                    id INT AUTO_INCREMENT PRIMARY KEY,
                    organization_id CHAR(36),
                    timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
                    source_ip VARCHAR(50),
                    device_name VARCHAR(100) DEFAULT 'Unknown',
                    domain VARCHAR(255),
                    protocol VARCHAR(20),
                    port VARCHAR(10) DEFAULT '53',
                    risk_score INT DEFAULT 0,
                    entropy FLOAT DEFAULT 0.0,
                    severity VARCHAR(20) DEFAULT 'LOW',
                    agent_id VARCHAR(100),
                    dst_ip VARCHAR(50) DEFAULT '-',
                    packet_size INT DEFAULT 0,
                    device_type VARCHAR(50) DEFAULT 'Unknown',
                    os_family VARCHAR(50) DEFAULT 'Unknown',
                    brand VARCHAR(50) DEFAULT 'Unknown',
                    mac_address VARCHAR(50) DEFAULT '-',
                    identity_confidence VARCHAR(20) DEFAULT 'low',
                    CONSTRAINT fk_traffic_org FOREIGN KEY (organization_id) REFERENCES organizations(id)
                )
            """)

            # 5. Activity Logs Table (Multi-tenant)
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS activity_logs (
                    id INT AUTO_INCREMENT PRIMARY KEY,
                    organization_id CHAR(36),
                    timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
                    username VARCHAR(50),
                    action VARCHAR(255),
                    ip_address VARCHAR(50),
                    severity VARCHAR(20) DEFAULT 'INFO',
                    CONSTRAINT fk_activity_org FOREIGN KEY (organization_id) REFERENCES organizations(id)
                )
            """)

            # 6. Device Aliases Table (Multi-tenant)
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS device_aliases (
                    id INT AUTO_INCREMENT PRIMARY KEY,
                    organization_id CHAR(36),
                    ip_address VARCHAR(50) NOT NULL,
                    device_name VARCHAR(100) NOT NULL,
                    updated_at DATETIME DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
                    UNIQUE(organization_id, ip_address),
                    CONSTRAINT fk_device_org FOREIGN KEY (organization_id) REFERENCES organizations(id)
                )
            """)

            # 7. Device Risk Table
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS device_risks (
                    device_id VARCHAR(100) PRIMARY KEY,
                    organization_id CHAR(36),
                    ip_address VARCHAR(50),
                    current_score FLOAT DEFAULT 0.0,
                    risk_level VARCHAR(20) DEFAULT 'LOW',
                    reasons TEXT,
                    last_updated DATETIME DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
                    CONSTRAINT fk_risk_org FOREIGN KEY (organization_id) REFERENCES organizations(id)
                )
            """)

            # 8. Device Baselines Table
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS device_baselines (
                    device_id VARCHAR(100) PRIMARY KEY,
                    organization_id CHAR(36),
                    avg_packet_rate FLOAT DEFAULT 0.0,
                    avg_dns_per_hour FLOAT DEFAULT 0.0,
                    avg_ports_used INT DEFAULT 0,
                    active_hours_pattern TEXT,
                    last_computed DATETIME DEFAULT CURRENT_TIMESTAMP,
                    CONSTRAINT fk_baseline_org FOREIGN KEY (organization_id) REFERENCES organizations(id)
                )
            """)

            # 9. Security Policies Table
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS security_policies (
                    organization_id CHAR(36) PRIMARY KEY,
                    blocked_domains TEXT,
                    vpn_restriction BOOLEAN DEFAULT FALSE,
                    alert_threshold INT DEFAULT 70,
                    updated_at DATETIME DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
                    CONSTRAINT fk_policy_org FOREIGN KEY (organization_id) REFERENCES organizations(id)
                )
            """)

            conn.commit()

            # --- HELPER: Ensure Columns exist ---
            def add_column_if_missing(table, col, col_def):
                cursor.execute(f"DESCRIBE {table}")
                existing = [c[0] for c in cursor.fetchall()]
                if col not in existing:
                    print(f"[*] Adding {col} to {table}...")
                    cursor.execute(f"ALTER TABLE {table} ADD COLUMN {col} {col_def}")

            add_column_if_missing("traffic_logs", "organization_id", "CHAR(36)")
            add_column_if_missing("activity_logs", "organization_id", "CHAR(36)")
            add_column_if_missing("device_aliases", "organization_id", "CHAR(36)")

            # Upgrade Users table ID to CHAR(36) if still INT
            cursor.execute("DESCRIBE users")
            user_cols = cursor.fetchall()
            id_col = next(c for c in user_cols if c[0] == 'id')
            if 'int' in id_col[1].lower():
                print("[!] Migrating users table to UUID IDs...")
                # This is tricky with FKs, but since we are just starting SaaS refactor, we can be aggressive
                cursor.execute("ALTER TABLE users MODIFY id CHAR(36)")
            
            add_column_if_missing("users", "organization_id", "CHAR(36)")

            conn.commit()

            # --- MIGRATION & BOOTSTRAP ---
            
            cursor.execute("SELECT id FROM organizations WHERE name = 'Default Organization' LIMIT 1")
            org_row = cursor.fetchone()
            if not org_row:
                default_org_id = str(uuid.uuid4())
                cursor.execute("INSERT INTO organizations (id, name, status) VALUES (%s, %s, %s)", 
                               (default_org_id, "Default Organization", "active"))
                conn.commit()
                print(f"{Fore.CYAN}[*] Created Default Organization: {default_org_id}")
            else:
                default_org_id = org_row[0]

            admin_user = os.environ.get("NETVISOR_BOOTSTRAP_ADMIN_USERNAME", "admin")
            admin_pass = os.environ.get("NETVISOR_BOOTSTRAP_ADMIN_PASSWORD")
            
            if admin_pass:
                cursor.execute("SELECT password, id FROM users WHERE username = %s", (admin_user,))
                user_row = cursor.fetchone()
                if not user_row:
                    hashed_pass = bcrypt.hashpw(admin_pass.encode(), bcrypt.gensalt()).decode()
                    cursor.execute("INSERT INTO users (id, username, password, role, organization_id) VALUES (%s, %s, %s, %s, %s)", 
                                   (str(uuid.uuid4()), admin_user, hashed_pass, "super_admin", default_org_id))
                    conn.commit()
                    print(f"{Fore.GREEN}[+] Super Admin bootstrapped.")
                else:
                    cursor.execute("UPDATE users SET role = 'super_admin', organization_id = %s WHERE username = %s AND organization_id IS NULL", 
                                   (default_org_id, admin_user))
                    conn.commit()

            # performance indexes
            try:
                cursor.execute("CREATE INDEX idx_traffic_org ON traffic_logs(organization_id)")
                cursor.execute("CREATE INDEX idx_time ON traffic_logs(timestamp)")
                cursor.execute("CREATE INDEX idx_user_org ON users(organization_id)")
            except: pass

            cursor.close()
            print(f"{Fore.GREEN}[!] Database initialized for Multi-Tenancy.")
        except Exception as e:
            print(f"{Fore.RED}[X] DB Init Error: {e}")
            import traceback
            traceback.print_exc()
        finally:
            if conn:
                try:
                    conn.close()
                except Exception as e:
                    print(f"DB Close Error: {e}")

# --- SINGLE WRITER DB BUFFER ---
packet_queue = asyncio.Queue(maxsize=10000)

async def drain_packet_queue():
    """Drains all remaining items in the queue to the database before shutdown."""
    print(f"{Fore.YELLOW}[!] Draining {packet_queue.qsize()} logs to DB...")
    while not packet_queue.empty():
        logs = []
        try:
            while len(logs) < 500:
                try:
                    logs.append(packet_queue.get_nowait())
                except asyncio.QueueEmpty:
                    break
            
            if logs:
                write_logs_to_db(logs)
                for _ in range(len(logs)):
                    packet_queue.task_done()
        except Exception as e:
            print(f"Drain error: {e}")
            break
    print(f"{Fore.GREEN}[+] Queue drained successfully.")

async def db_writer_worker():
    while True:
        logs = []
        log = await packet_queue.get()
        logs.append(log)
        
        while len(logs) < 100:
            try:
                log = packet_queue.get_nowait()
                logs.append(log)
            except asyncio.QueueEmpty:
                break
        
        if logs:
            loop = asyncio.get_event_loop()
            await loop.run_in_executor(None, write_logs_to_db, logs)
            
            # Always mark tasks as done
            for _ in range(len(logs)):
                packet_queue.task_done()

def write_logs_to_db(logs):
    from services.detector import detector
    conn = get_db_connection()
    if conn:
        try:
            cursor = conn.cursor()
            
            # Process logs through detection engine before saving
            enriched_logs = []
            for l in logs:
                # Dynamic detection
                score, entropy, severity, ml_prob, reasons = detector.analyze_packet(
                    domain=l.domain,
                    src_ip=l.src_ip,
                    dst_ip=l.dst_ip,
                    port=l.port,
                    device_id=l.mac_address # Using MAC as device ID for now
                )
                l.risk_score = int(score)
                l.entropy = entropy
                l.severity = severity
                enriched_logs.append(l)

            sql = "INSERT INTO traffic_logs (timestamp, source_ip, dst_ip, device_name, domain, protocol, port, risk_score, entropy, severity, agent_id, packet_size, device_type, os_family, brand, mac_address, identity_confidence, organization_id) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)"
            vals = [(
                l.time, l.src_ip, l.dst_ip, l.device_name or "Unknown", l.domain, l.protocol, l.port,
                l.risk_score, l.entropy, (l.severity or "LOW").upper(), l.agent_id, l.size,
                l.device_type or "Unknown", l.os_family or "Unknown", l.brand or "Unknown",
                l.mac_address or "-", l.identity_confidence or "low", l.organization_id
            ) for l in enriched_logs]
            cursor.executemany(sql, vals)
            
            # Update Device Risk Table
            for l in enriched_logs:
                if l.mac_address and l.mac_address != "-":
                    cursor.execute("""
                        INSERT INTO device_risks (device_id, organization_id, ip_address, current_score, risk_level, reasons)
                        VALUES (%s, %s, %s, %s, %s, %s)
                        ON DUPLICATE KEY UPDATE 
                            current_score = VALUES(current_score),
                            risk_level = VALUES(risk_level),
                            reasons = VALUES(reasons),
                            ip_address = VALUES(ip_address)
                    """, (l.mac_address, l.organization_id, l.src_ip, l.risk_score, l.severity, ",".join(reasons)))

            conn.commit()
            cursor.close()
        except Exception as e:
            print(f"DB Worker Error: {e}")
        finally:
            conn.close()
            
            # Always mark tasks as done, even if DB write failed (logs dropped)
            for _ in range(len(logs)):
                packet_queue.task_done()

# --- DATA ACCESS LAYER ---

def db_fetch_recent_traffic(limit=1000, severity=None, organization_id=None):
    conn = get_db_connection()
    if not conn: return []
    try:
        cursor = conn.cursor(dictionary=True)
        query = "SELECT * FROM traffic_logs"
        params = []
        conditions = []
        
        if organization_id:
            conditions.append("organization_id = %s")
            params.append(organization_id)
        if severity:
            conditions.append("severity = %s")
            params.append(severity)
            
        if conditions:
            query += " WHERE " + " AND ".join(conditions)
            
        query += " ORDER BY id DESC LIMIT %s"
        params.append(limit)
        
        cursor.execute(query, tuple(params))
        return cursor.fetchall()
    except Exception as e:
        print(f"DB Fetch Error: {e}")
        return []
    finally:
        if conn: conn.close()

def db_fetch_system_logs(limit=50, organization_id=None):
    conn = get_db_connection()
    if not conn: return []
    try:
        cursor = conn.cursor(dictionary=True)
        query = "SELECT * FROM activity_logs"
        params = []
        if organization_id:
            query += " WHERE organization_id = %s"
            params.append(organization_id)
        query += " ORDER BY id DESC LIMIT %s"
        params.append(limit)
        
        cursor.execute(query, tuple(params))
        return cursor.fetchall()
    except Exception as e:
        print(f"Stats Fetch Error: {e}")
        return []
    finally:
        if conn: conn.close()

def db_fetch_vpn_alerts(limit=50, organization_id=None):
    conn = get_db_connection()
    if not conn: return []
    try:
        cursor = conn.cursor(dictionary=True)
        query = "SELECT * FROM traffic_logs WHERE (severity='HIGH' OR risk_score > 70)"
        params = []
        if organization_id:
            query += " AND organization_id = %s"
            params.append(organization_id)
        query += " ORDER BY id DESC LIMIT %s"
        params.append(limit)
        
        cursor.execute(query, tuple(params))
        return cursor.fetchall()
    except Exception as e:
        print(f"VPN Fetch Error: {e}")
        return []
    finally:
        if conn: conn.close()

def db_fetch_device_risks(organization_id=None):
    conn = get_db_connection()
    if not conn: return []
    try:
        cursor = conn.cursor(dictionary=True)
        query = "SELECT * FROM device_risks"
        params = []
        if organization_id:
            query += " WHERE organization_id = %s"
            params.append(organization_id)
        
        cursor.execute(query, tuple(params))
        return cursor.fetchall()
    except Exception as e:
        print(f"DB Fetch Risks Error: {e}")
        return []
    finally:
        if conn: conn.close()

def db_truncate_tables():
    conn = get_db_connection()
    if not conn: return False
    try:
        cursor = conn.cursor()
        cursor.execute("TRUNCATE TABLE traffic_logs")
        cursor.execute("TRUNCATE TABLE activity_logs")
        conn.commit()
        return True
    except Exception as e:
        print(f"Truncate Error: {e}")
        return False
    finally:
        if conn: conn.close()

def db_export_to_csv():
    import csv
    import datetime
    
    if not os.path.exists("data/backups"):
        os.makedirs("data/backups")
        
    conn = get_db_connection()
    if not conn: return None
    try:
        cursor = conn.cursor(dictionary=True)
        cursor.execute("SELECT * FROM traffic_logs")
        rows = cursor.fetchall()
        if not rows: return "empty"
        
        filename = f"data/backups/traffic_export_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
        with open(filename, 'w', newline='') as f:
            writer = csv.DictWriter(f, fieldnames=rows[0].keys())
            writer.writeheader()
            writer.writerows(rows)
        return filename
    except Exception as e:
        print(f"Export Error: {e}")
        return None
    finally:
        if conn: conn.close()


==================================================

### FILE: core\models.py ###
from pydantic import BaseModel
from typing import Optional, List

class PacketLog(BaseModel):
    time: str
    src_ip: str
    dst_ip: Optional[str] = "-"
    domain: str
    protocol: Optional[str] = "DNS"
    port: Optional[str] = "53"
    risk_score: Optional[int] = 0
    entropy: Optional[float] = 0.0
    severity: Optional[str] = "LOW"
    size: Optional[int] = 0
    agent_id: Optional[str] = "GATEWAY_SENSE_01"
    organization_id: str
    device_name: Optional[str] = "Unknown"
    device_type: Optional[str] = "Unknown"
    os_family: Optional[str] = "Unknown"
    brand: Optional[str] = "Unknown"
    mac_address: Optional[str] = "-"
    identity_confidence: Optional[str] = "low"

class DeviceRisk(BaseModel):
    device_id: str
    ip_address: str
    current_score: float
    risk_level: str # LOW, MEDIUM, HIGH
    reasons: List[str]
    last_updated: str
    organization_id: str

class DeviceBaseline(BaseModel):
    device_id: str
    avg_packet_rate: float
    avg_dns_per_hour: float
    avg_ports_used: int
    active_hours_pattern: str # JSON-encoded or simple string description
    last_computed: str
    organization_id: str

# --- API REQUEST SCHEMAS ---

class UserLogin(BaseModel):
    username: str
    password: str

class UserRegister(BaseModel):
    username: str
    email: str
    password: str
    confirm_password: str
    role: Optional[str] = "viewer"

class HotspotRequest(BaseModel):
    action: str

class SystemConfigRequest(BaseModel):
    active: bool

class AgentRegistration(BaseModel):
    agent_id: str
    os: str
    hostname: str
    version: str
    time: str
    organization_id: str

class AgentHeartbeat(BaseModel):
    agent_id: str
    status: str
    dropped_packets: int
    time: str
    organization_id: str
    cpu_usage: Optional[float] = 0.0
    ram_usage: Optional[float] = 0.0
    inventory_size: Optional[int] = 0

# --- API RESPONSE SCHEMAS ---

class GenericResponse(BaseModel):
    status: str
    message: Optional[str] = None

class DeviceResponse(BaseModel):
    ip: str
    mac: str
    hostname: str
    traffic: float
    is_online: bool
    last_seen: str
    type: str
    os: str
    brand: str
    confidence: str
    risk_score: Optional[int] = 0
    risk_level: Optional[str] = "LOW"

class SystemHealthResponse(BaseModel):
    status: str
    cpu_usage: float
    ram_usage: float
    uptime_hours: float

class AdminStatsResponse(BaseModel):
    hostname: str
    local_ip: str
    cpu_percent: float
    mem_used_mb: float
    mem_total_mb: float
    maintenance_mode: bool

class ActivityEntry(BaseModel):
    time: str
    ip: str
    dst_ip: str
    domain: str
    protocol: str
    size: int
    device: str
    os: str
    brand: str
    mac: str
    confidence: str
    severity: str

class AdminLogEntry(BaseModel):
    time: str
    action: str
    details: str

class VPNLogEntry(BaseModel):
    time: str
    src_ip: str
    score: float
    reason: str

class PolicyUpdate(BaseModel):
    blocked_domains: List[str]
    vpn_restriction: bool
    alert_threshold: int
    organization_id: str

class LogsResponse(BaseModel):
    admin: List[AdminLogEntry]
    vpn: List[VPNLogEntry]


==================================================

### FILE: core\security.py ###
import bcrypt
import hashlib

def hash_password(password: str) -> str:
    return bcrypt.hashpw(password.encode(), bcrypt.gensalt()).decode()

def verify_password(plain_password: str, hashed_password: str) -> bool:
    # Try bcrypt first
    try:
        if bcrypt.checkpw(plain_password.encode(), hashed_password.encode()):
            return True
    except:
        pass
    
    # Legacy SHA256 fallback
    if len(hashed_password) == 64:
        sha256_hash = hashlib.sha256(plain_password.encode()).hexdigest()
        if sha256_hash == hashed_password:
            return True
            
    return False


==================================================

### FILE: core\state.py ###

class AppState:
    def __init__(self):
        self.maintenance_mode = False
        self.monitoring_active = True
        self.hotspot_active = False
        self.start_time = 0

state = AppState()


==================================================

### FILE: services\baseline_service.py ###
from core.database import get_db_connection
import datetime
import json

class BaselineService:
    def __init__(self):
        pass

    def compute_all_baselines(self):
        """
        Computes baselines for all active devices based on the last 24 hours of traffic.
        """
        conn = get_db_connection()
        if not conn: return
        
        try:
            cursor = conn.cursor(dictionary=True)
            
            # 1. Identify active devices and calculate averages
            # We look at: Avg Packets per Minute, Avg Unique Domains per Hour
            cursor.execute("""
                SELECT 
                    mac_address, 
                    organization_id,
                    COUNT(*) / (24 * 60) as avg_packet_rate,
                    COUNT(DISTINCT domain) / 24 as avg_dns_per_hour,
                    COUNT(DISTINCT port) as avg_ports_used
                FROM traffic_logs
                WHERE timestamp > (NOW() - INTERVAL 1 DAY)
                AND mac_address != '-'
                GROUP BY mac_address, organization_id
            """)
            
            baselines = cursor.fetchall()
            
            for b in baselines:
                # 2. Update or Insert into device_baselines
                cursor.execute("""
                    INSERT INTO device_baselines (device_id, organization_id, avg_packet_rate, avg_dns_per_hour, avg_ports_used, last_computed)
                    VALUES (%s, %s, %s, %s, %s, NOW())
                    ON DUPLICATE KEY UPDATE
                        avg_packet_rate = VALUES(avg_packet_rate),
                        avg_dns_per_hour = VALUES(avg_dns_per_hour),
                        avg_ports_used = VALUES(avg_ports_used),
                        last_computed = NOW()
                """, (b['mac_address'], b['organization_id'], b['avg_packet_rate'], b['avg_dns_per_hour'], b['avg_ports_used']))
            
            conn.commit()
            print(f"[+] Computed baselines for {len(baselines)} devices.")
            
        except Exception as e:
            print(f"[-] Baseline Compute Error: {e}")
        finally:
            conn.close()

baseline_service = BaselineService()


==================================================

### FILE: services\detector.py ###
from backend.detection.risk_engine import risk_engine
from core.database import get_db_connection

class AnomalyDetector:
    def __init__(self, risk_threshold=70):
        self.risk_threshold = risk_threshold

    def get_device_baseline(self, device_id):
        """Fetch baseline from DB if available"""
        conn = get_db_connection()
        if not conn: return None
        try:
            cursor = conn.cursor(dictionary=True)
            cursor.execute("SELECT * FROM device_baselines WHERE device_id = %s", (device_id,))
            row = cursor.fetchone()
            return row
        except:
            return None
        finally:
            conn.close()

    def analyze_packet(self, domain, src_ip, dst_ip="-", port="53", rcode=0, device_id=None):
        """
        Delegates detection to the centralized RiskEngine.
        Returns: (total_score, entropy, severity, ml_prob, reasons)
        """
        # 1. Try to get baseline for the device
        baseline = None
        if device_id:
            baseline = self.get_device_baseline(device_id)

        # 2. Run Risk Engine
        report = risk_engine.calculate_device_risk(
            domain=domain, 
            src_ip=src_ip, 
            dst_ip=dst_ip, 
            port=port, 
            rcode=rcode, 
            baseline=baseline
        )

        return (
            report["score"], 
            report["entropy"], 
            report["severity"], 
            report["ml_prob"],
            report["reasons"]
        )

# Global Instance
detector = AnomalyDetector()

==================================================

### FILE: services\hostname_resolver.py ###
import socket
import logging
import subprocess

logger = logging.getLogger(__name__)

class HostnameResolver:
    @staticmethod
    def resolve(ip_address: str) -> str:
        """Resolve hostname via reverse DNS and NetBIOS."""
        try:
            # 1. Reverse DNS
            hostname, _, _ = socket.gethostbyaddr(ip_address)
            return hostname
        except (socket.herror, socket.gaierror):
            pass

        # 2. NetBIOS fallback (Windows)
        try:
            output = subprocess.check_output(f"nbtstat -A {ip_address}", shell=True, timeout=2).decode(errors="ignore")
            for line in output.split('\n'):
                if "<00>" in line and "UNIQUE" in line:
                    parts = line.split()
                    return parts[0].strip()
        except:
            pass

        return "Unknown"

==================================================

### FILE: services\vendor_resolver.py ###
import requests
import logging

logger = logging.getLogger(__name__)

class VendorResolver:
    OUI_API_URL = "https://api.macvendors.com/"
    
    def __init__(self):
        self.cache = {}
        # Common OUIs (Pre-loaded)
        self.cache_init = {
            "00:50:56": "VMware",
            "00:0C:29": "VMware",
            "00:05:69": "VMware",
            "00:1C:14": "VMware",
            "08:00:27": "Oracle VirtualBox",
            "00:15:5D": "Microsoft Hyper-V",
        }
        self.cache.update(self.cache_init)

    def resolve(self, mac_address: str) -> str:
        """Resolve vendor from MAC address."""
        if not mac_address or mac_address == "-" or len(mac_address) < 8:
            return "Unknown"
            
        mac_address = mac_address.replace("-", ":").upper()
        
        prefix = mac_address[:8]
        if prefix in self.cache:
            return self.cache[prefix]

        try:
            # Note: Public API rate limits may apply. 
            # In production, use local database or paid API key.
            response = requests.get(f"{self.OUI_API_URL}{mac_address}", timeout=2)
            if response.status_code == 200:
                vendor = response.text.strip()
                self.cache[prefix] = vendor
                return vendor
        except:
            pass
            
        return "Unknown"

==================================================

### FILE: services\ml\ml_engine.py ###
import os
import pickle
import math
import numpy as np
import threading
from collections import Counter
from sklearn.ensemble import RandomForestClassifier
from sklearn.exceptions import NotFittedError

class DNSThreatClassifier:
    def __init__(self, model_path="services/ml/models/dns_threat_model.pkl"):
        self.model_path = model_path
        self.model = None
        self.lock = threading.Lock()
        
        # Ensure model directory exists
        os.makedirs(os.path.dirname(self.model_path), exist_ok=True)
        
        self.load_model()

    def load_model(self):
        if os.path.exists(self.model_path):
            try:
                with open(self.model_path, "rb") as f:
                    self.model = pickle.load(f)
                    print("[+] ML Model Loaded Successfully")
            except Exception as e:
                print(f"[-] Failed to load model: {e}")
        else:
            print("[!] No model found. Training initial model...")
            self.train_initial_model()

    def extract_features(self, domain):
        """Extract lexical features from domain"""
        if not domain: return [0, 0, 0, 0, 0]
        
        domain = domain.lower()
        parts = domain.split('.')
        main_part = parts[0] if parts else domain
        
        length = len(main_part)
        entropy = self.calculate_entropy(main_part)
        digit_count = sum(c.isdigit() for c in main_part)
        digit_ratio = digit_count / length if length > 0 else 0
        vowel_count = sum(1 for c in main_part if c in 'aeiou')
        vowel_ratio = vowel_count / length if length > 0 else 0
        subdomain_count = len(parts) - 1
        
        # New features
        consecutive_consonants = self._max_consecutive_consonants(main_part)
        
        return [length, entropy, digit_ratio, vowel_ratio, subdomain_count, consecutive_consonants]

    def _max_consecutive_consonants(self, text):
        max_c = 0
        current = 0
        vowels = set('aeiou')
        for char in text:
            if char.isalpha() and char not in vowels:
                current += 1
                max_c = max(max_c, current)
            else:
                current = 0
        return max_c

    def calculate_entropy(self, text):
        if not text: return 0
        counter = Counter(text)
        total = len(text)
        return -sum((count/total) * math.log2(count/total) for count in counter.values())

    def train_initial_model(self):
        """Train a lightweight initial model avoiding external dependencies if possible"""
        # Feature Vector: [length, entropy, digit_ratio, vowel_ratio, subdomain_count]
        
        # Benign Samples (Google, Facebook, etc.)
        X_benign = [
            [6, 1.9, 0.0, 0.5, 1, 1], # google
            [8, 2.5, 0.0, 0.5, 1, 2], # facebook
            [7, 2.8, 0.0, 0.3, 1, 2], # youtube
            [6, 2.6, 0.0, 0.3, 1, 1], # amazon
            [9, 2.9, 0.0, 0.4, 2, 3], # wikipedia
            [4, 2.0, 0.0, 0.5, 1, 1], # bing
        ]
        y_benign = [0] * len(X_benign)

        # Malicious/DGA Samples (High entropy, random digits)
        X_malicious = [
            [15, 3.8, 0.2, 0.1, 1, 6], # a1b2c3d4e5f6g7
            [20, 4.2, 0.3, 0.1, 1, 8], # 9876543210qwerty
            [12, 3.5, 0.0, 0.0, 3, 7], # xklqwpzjv.com
            [18, 3.9, 0.5, 0.1, 2, 9], # 1234567890abcdef.net
            [25, 4.5, 0.1, 0.1, 4, 10], # super-long-random-string
        ]
        y_malicious = [1] * len(X_malicious)

        X = np.array(X_benign + X_malicious)
        y = np.array(y_benign + y_malicious)

        clf = RandomForestClassifier(n_estimators=10, max_depth=5, random_state=42)
        clf.fit(X, y)
        
        with self.lock:
            self.model = clf
            with open(self.model_path, "wb") as f:
                pickle.dump(clf, f)
        print("[+] Initial ML Model Trained & Saved")

    def predict_risk(self, domain):
        """Returns risk probability (0.0 - 1.0)"""
        if not self.model: return 0.0
        
        try:
            features = np.array([self.extract_features(domain)])
            with self.lock:
                prob = self.model.predict_proba(features)[0][1] # Probability of class 1 (Malicious)
            return prob
        except Exception as e:
            # print(f"Prediction Error: {e}")
            return 0.0

# Singleton Instance
ml_engine = DNSThreatClassifier()

==================================================

### FILE: services\ml\vpn_ml.py ###
class VPNAnomalyDetector:
    def __init__(self):
        pass
    
    def predict(self, features):
        # Placeholder for VPN anomaly detection logic
        return 0.0

==================================================

